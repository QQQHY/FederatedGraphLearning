<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Graph Learning &mdash; Federated Graph Learning 0.01 文档</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="Federated Graph Learning Paper List" href="FGL.html" />
    <link rel="prev" title="Federated Graph Learning Resources" href="../FL.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Federated Graph Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Paper List</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../FL.html">Federated Graph Learning Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="../FL.html#others">Others</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Graph Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#survey">Survey</a></li>
<li class="toctree-l3"><a class="reference internal" href="#homogeneous-gnn">Homogeneous GNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="#heterogeneous-gnn">Heterogeneous GNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="#privacy-preserving">Privacy-Preserving</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="FGL.html">Federated Graph Learning Paper List</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Federated Graph Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Paper List</a> &raquo;</li>
      <li>Graph Learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/FederatedGraphLearning/GL.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="graph-learning">
<h1>Graph Learning<a class="headerlink" href="#graph-learning" title="永久链接至标题"></a></h1>
<p>Generally sorted by date.</p>
<hr class="docutils" />
<section id="survey">
<h2>Survey<a class="headerlink" href="#survey" title="永久链接至标题"></a></h2>
<ul class="simple">
<li><p><strong>A Comprehensive Survey on Graph Neural Networks</strong></p>
<ul>
<li><p>Author:</p></li>
<li><p>Publication:</p></li>
<li><p>Date:</p></li>
<li><p>Link:</p></li>
<li><p>Abstract:</p></li>
<li><p>Note：IEEE TNNLS 2020</p></li>
</ul>
</li>
<li><p><strong>Deep Learning on Graphs: A Survey</strong></p>
<ul>
<li><p>Author:</p></li>
<li><p>Publication:</p></li>
<li><p>Date:</p></li>
<li><p>Link:</p></li>
<li><p>Abstract:</p></li>
<li><p>Note：IEEE TKDE 2020</p></li>
</ul>
</li>
<li><p><strong>Graph Neural Networks: A Review of Methods and Applications</strong></p>
<ul>
<li><p>Author:</p></li>
<li><p>Publication:</p></li>
<li><p>Date:</p></li>
<li><p>Link:</p></li>
<li><p>Abstract:</p></li>
<li><p>Note：</p></li>
</ul>
</li>
</ul>
</section>
<section id="homogeneous-gnn">
<h2>Homogeneous GNN<a class="headerlink" href="#homogeneous-gnn" title="永久链接至标题"></a></h2>
<ul class="simple">
<li><p><strong>Semi-Supervised Classification with Graph Convolutional Networks</strong></p>
<ul>
<li><p>Author:</p></li>
<li><p>Publication: ICLR 2017</p></li>
<li><p>Date:</p></li>
<li><p>Link:</p></li>
<li><p>Abstract:</p></li>
<li><p>Note：GCN</p></li>
</ul>
</li>
<li><p><strong>Inductive representation learning on large graphs</strong></p>
<ul>
<li><p>Author:</p></li>
<li><p>Publication: NeurIPS 2017</p></li>
<li><p>Date:</p></li>
<li><p>Link:</p></li>
<li><p>Abstract:</p></li>
<li><p>Note: GraphSAGE</p></li>
</ul>
</li>
<li><p><strong>Graph attention networks</strong></p>
<ul>
<li><p>Author:</p></li>
<li><p>Publication: ICLR 2018</p></li>
<li><p>Date:</p></li>
<li><p>Link:</p></li>
<li><p>Abstract:</p></li>
<li><p>Note: GAT</p></li>
</ul>
</li>
<li><p><strong>Subgraph Neural Networks</strong></p>
<ul>
<li><p>Author: Emily Alsentzer, Samuel G. Finlayson, Michelle M. Li, Marinka Zitnik</p></li>
<li><p>Publication: NeurIPS 2020</p></li>
<li><p>Date: 18 Jun 2020</p></li>
<li><p>Link: <a class="reference external" href="https://arxiv.org/abs/2006.10538">https://arxiv.org/abs/2006.10538</a></p></li>
<li><p>Abstract: Deep learning methods for graphs achieve remarkable performance on many node-level and graph-level prediction tasks. However, despite the proliferation of the methods and their success, prevailing Graph Neural Networks (GNNs) neglect subgraphs, rendering subgraph prediction tasks challenging to tackle in many impactful applications. Further, subgraph prediction tasks present several unique challenges: subgraphs can have non-trivial internal topology, but also carry a notion of position and external connectivity information relative to the underlying graph in which they exist. Here, we introduce SubGNN, a subgraph neural network to learn disentangled subgraph representations. We propose a novel subgraph routing mechanism that propagates neural messages between the subgraph’s components and randomly sampled anchor patches from the underlying graph, yielding highly accurate subgraph representations. SubGNN specifies three channels, each designed to capture a distinct aspect of subgraph topology, and we provide empirical evidence that the channels encode their intended properties. We design a series of new synthetic and real-world subgraph datasets. Empirical results for subgraph classification on eight datasets show that SubGNN achieves considerable performance gains, outperforming strong baseline methods, including node-level and graph-level GNNs, by 19.8% over the strongest baseline. SubGNN performs exceptionally well on challenging biomedical datasets where subgraphs have complex topology and even comprise multiple disconnected components.</p></li>
<li><p>Note</p>
<ul>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/337203750">Subgraph Neural Networks 论文阅读</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="heterogeneous-gnn">
<h2>Heterogeneous GNN<a class="headerlink" href="#heterogeneous-gnn" title="永久链接至标题"></a></h2>
<ul class="simple">
<li><p><strong>Modeling Relational Data with Graph Convolutional Networks</strong></p>
<ul>
<li><p>Author: Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, Max Welling</p></li>
<li><p>Publication: ESWC 2018</p></li>
<li><p>Date: 17 Mar 2017</p></li>
<li><p>Link: <a class="reference external" href="https://arxiv.org/abs/1703.06103">https://arxiv.org/abs/1703.06103</a></p></li>
<li><p>Abstract: Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.</p></li>
<li><p><a class="reference external" href="https://docs.google.com/presentation/d/1_Tm8IxCUz4ARI3kcMvLNstRzhrBKbsp5dZNwjVwxA04/edit?usp=sharing">Meeting_20211103_R-GCN</a></p></li>
<li><p><a class="reference external" href="https://paperswithcode.com/paper/modeling-relational-data-with-graph">Paper With Code</a></p>
<ul>
<li><p><a class="reference external" href="https://github.com/masakicktashiro/rgcn_pytorch_implementation">Graph Convolutional Networks for relational graphs</a></p></li>
<li><p><a class="reference external" href="https://github.com/thiviyanT/torch-rgcn">Torch-RGCN</a></p></li>
<li><p><a class="reference external" href="https://github.com/berlincho/RGCN-pytorch">Relational Graph Convolutional Networks (RGCN) Pytorch implementation</a></p></li>
</ul>
</li>
<li><p>Note</p>
<ul>
<li><p>Heterogeneous graph neural network</p></li>
<li><p>R-GCN</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Heterogeneous graph attention network</strong></p>
<ul>
<li><p>Author: Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Peng Cui, P. Yu, Yanfang Ye</p></li>
<li><p>Publication: WWW 2019</p></li>
<li><p>Date: 18 Mar 2019</p></li>
<li><p>Link: <a class="reference external" href="https://arxiv.org/abs/1903.07293">https://arxiv.org/abs/1903.07293</a></p></li>
<li><p>Abstract: Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its metapath based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis.</p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/346658317">HAN详解（Heterogeneous graph attention network）</a></p></li>
</ul>
</li>
<li><p><strong>Heterogeneous Graph Neural Network</strong></p>
<ul>
<li><p>Author: Zhang, Chuxu and Song, Dongjin and Huang, Chao and Swami, Ananthram and Chawla, Nitesh V.</p></li>
<li><p>Publication: SIGKDD 2019</p></li>
<li><p>Date: 25 July 2019</p></li>
<li><p>Link: <a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3292500.3330961">https://dl.acm.org/doi/abs/10.1145/3292500.3330961</a></p></li>
<li><p>Abstract: Representation learning in heterogeneous graphs aims to pursue a meaningful vector representation for each node so as to facilitate downstream applications such as link prediction, personalized recommendation, node classification, etc. This task, however, is challenging not only because of the demand to incorporate heterogeneous structural (graph) information consisting of multiple types of nodes and edges, but also due to the need for considering heterogeneous attributes or contents (e.g., text or image) associated with each node. Despite a substantial amount of effort has been made to homogeneous (or heterogeneous) graph embedding, attributed graph embedding as well as graph neural networks, few of them can jointly consider heterogeneous structural (graph) information as well as heterogeneous contents information of each node effectively. In this paper, we propose HetGNN, a heterogeneous graph neural network model, to resolve this issue. Specifically, we first introduce a random walk with restart strategy to sample a fixed size of strongly correlated heterogeneous neighbors for each node and group them based upon node types. Next, we design a neural network architecture with two modules to aggregate feature information of those sampled neighboring nodes. The first module encodes “deep” feature interactions of heterogeneous contents and generates content embedding for each node. The second module aggregates content (attribute) embeddings of different neighboring groups (types) and further combines them by considering the impacts of different groups to obtain the ultimate node embedding. Finally, we leverage a graph context loss and a mini-batch gradient descent procedure to train the model in an end-to-end manner. Extensive experiments on several datasets demonstrate that HetGNN can outperform state-of-the-art baselines in various graph mining tasks, i.e., link prediction, recommendation, node classification &amp; clustering and inductive node classification &amp; clustering.</p></li>
</ul>
</li>
<li><p><strong>Graph transformer networks</strong></p>
<ul>
<li><p>Author: Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim</p></li>
<li><p>Publication: NeurIPS’19</p></li>
<li><p>Date: 6 Nov 2019</p></li>
<li><p>Link: <a class="reference external" href="https://arxiv.org/abs/1911.06455">https://arxiv.org/abs/1911.06455</a></p></li>
<li><p>Abstract: Graph neural networks (GNNs) have been widely used in representation learning on graphs and achieved state-of-the-art performance in tasks such as node classification and link prediction. However, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. In this paper, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which involve identifying useful connections between unconnected nodes on the original graph, while learning effective node representation on the new graphs in an end-to-end fashion. Graph Transformer layer, a core layer of GTNs, learns a soft selection of edge types and composite relations for generating useful multi-hop connections so-called meta-paths. Our experiments show that GTNs learn new graph structures, based on data and tasks without domain knowledge, and yield powerful node representation via convolution on the new graphs. Without domain-specific graph preprocessing, GTNs achieved the best performance in all three benchmark node classification tasks against the state-of-the-art methods that require pre-defined meta-paths from domain knowledge.</p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/365129455">Graph Transformer——合理灌水</a></p></li>
<li><p><a class="reference external" href="https://paperswithcode.com/paper/graph-transformer-networks-1">Papers With Code</a></p></li>
</ul>
</li>
<li><p><strong>Relation structure-aware heterogeneous graph neural network</strong></p>
<ul>
<li><p>Author: Shichao Zhu, Chuan Zhou, <a class="reference external" href="https://shiruipan.github.io/">Shirui Pan</a>, Xingquan Zhu, Bin Wang</p></li>
<li><p>Publication: ICDM 2019</p></li>
<li><p>Date: 8 Nov 2019</p></li>
<li><p>Link: <a class="reference external" href="https://ieeexplore.ieee.org/document/8970828">https://ieeexplore.ieee.org/document/8970828</a></p></li>
<li><p>Abstract: Heterogeneous graphs with different types of nodes and edges are ubiquitous and have immense value in many applications. Existing works on modeling heterogeneous graphs usually follow the idea of splitting a heterogeneous graph into multiple homogeneous subgraphs. This is ineffective in exploiting hidden rich semantic associations between different types of edges for large-scale multi-relational graphs. In this paper, we propose Relation Structure-Aware Heterogeneous Graph Neural Network (RSHN), a unified model that integrates graph and its coarsened line graph to embed both nodes and edges in heterogeneous graphs without requiring any prior knowledge such as metapath. To tackle the heterogeneity of edge connections, RSHN first creates a Coarsened Line Graph Neural Network (CL-GNN) to excavate edge-centric relation structural features that respect the latent associations of different types of edges based on coarsened line graph. After that, a Heterogeneous Graph Neural Network (H-GNN) is used to leverage implicit messages from neighbor nodes and edges propagating among nodes in heterogeneous graphs. As a result, different types of nodes and edges can enhance their embedding through mutual integration and promotion. Experiments and comparisons, based on semi-supervised classification tasks on large scale heterogeneous networks with over a hundred types of edges, show that RSHN significantly outperforms state-of-the-arts.</p></li>
<li><p><a class="reference external" href="https://github.com/CheriseZhu/RSHN">Code</a></p></li>
<li><p><a class="reference external" href="https://shiruipan.github.io/publication/icdm-19-zhu/">Paper Page</a></p></li>
</ul>
</li>
<li><p><strong>MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding</strong></p>
<ul>
<li><p>Author: Xinyu Fu, Jiani Zhang, Ziqiao Meng, Irwin King</p></li>
<li><p>Publication: WWW 2020</p></li>
<li><p>Date: 5 Feb 2020</p></li>
<li><p>Link: <a class="reference external" href="https://arxiv.org/abs/2002.01680">https://arxiv.org/abs/2002.01680</a></p></li>
<li><p>Abstract: A large number of real-world graphs or networks are inherently heterogeneous, involving a diversity of node types and relation types. Heterogeneous graph embedding is to embed rich structural and semantic information of a heterogeneous graph into low-dimensional node representations. Existing models usually define multiple metapaths in a heterogeneous graph to capture the composite relations and guide neighbor selection. However, these models either omit node content features, discard intermediate nodes along the metapath, or only consider one metapath. To address these three limitations, we propose a new model named Metapath Aggregated Graph Neural Network (MAGNN) to boost the final performance. Specifically, MAGNN employs three major components, i.e., the node content transformation to encapsulate input node attributes, the intra-metapath aggregation to incorporate intermediate semantic nodes, and the inter-metapath aggregation to combine messages from multiple metapaths. Extensive experiments on three real-world heterogeneous graph datasets for node classification, node clustering, and link prediction show that MAGNN achieves more accurate prediction results than state-of-the-art baselines.</p></li>
<li><p>Code <a class="reference external" href="https://github.com/cynricfu/MAGNN">MAGNN</a></p></li>
</ul>
</li>
<li><p><strong>Heterogeneous Graph Transformer</strong></p>
<ul>
<li><p>Author: Ziniu Hu, Yuxiao Dong, Kuansan Wang, Yizhou Sun</p></li>
<li><p>Publication: WWW 2020</p></li>
<li><p>Date: 3 Mar 2020</p></li>
<li><p>Link: <a class="reference external" href="https://arxiv.org/abs/2003.01332">https://arxiv.org/abs/2003.01332</a></p></li>
<li><p>Abstract: Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making them infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce the relative temporal encoding technique into HGT, which is able to capture the dynamic structural dependency with arbitrary durations. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm—HGSampling—for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9%–21% on various downstream tasks.</p></li>
<li><p>Code <a class="reference external" href="https://github.com/acbull/pyHGT">Heterogeneous Graph Transformer (HGT)</a></p></li>
</ul>
</li>
<li><p><strong>Are we really making much progress?: Revisiting, benchmarking and refining heterogeneous graph neural networks</strong></p>
<ul>
<li><p>Author: Qingsong Lv, Ming Ding, Qiang Liu, Yuxiang Chen, Wenzheng Feng, Siming
He, Chang Zhou, Jianguo Jiang, Yuxiao Dong, Jie Tang. 2</p></li>
<li><p>Publication: KDD 2021</p></li>
<li><p>Date: 14 August 2021</p></li>
<li><p>Link: <a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3447548.3467350">https://dl.acm.org/doi/abs/10.1145/3447548.3467350</a></p></li>
<li><p>Abstract: Heterogeneous graph neural networks (HGNNs) have been blossoming in recent years, but the unique data processing and evaluation setups used by each work obstruct a full understanding of their advancements. In this work, we present a systematical reproduction of 12 recent HGNNs by using their official codes, datasets, settings, and hyperparameters, revealing surprising findings about the progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and GAT, are largely underestimated due to improper settings. GAT with proper inputs can generally match or outperform all existing HGNNs across various scenarios. To facilitate robust and reproducible HGNN research, we construct the Heterogeneous Graph Benchmark (HGB) , consisting of 11 diverse datasets with three tasks. HGB standardizes the process of heterogeneous graph data splits, feature processing, and performance evaluation. Finally, we introduce a simple but very strong baseline Simple-HGN-which significantly outperforms all previous models on HGB-to accelerate the advancement of HGNNs in the future.</p></li>
<li><p><a class="reference external" href="https://keg.cs.tsinghua.edu.cn/jietang/publications/KDD21-slides-Lv-et-al-HeterGNN.pdf">Slides</a></p></li>
<li><p><a class="reference external" href="https://github.com/THUDM/HGB">Code &amp; Data Heterogeneous Graph Benchmark</a></p></li>
<li><p>CSDN Blog <a class="reference external" href="https://blog.csdn.net/qq_36291847/article/details/120712639">【KDD2021】Are we really making much progress? Revisiting, benchmarking, and refining HGNNs</a></p></li>
<li><p>异构图神经蓬勃发展但网络数据处理和评估设置存在差异。本文发现简单的同构GNNs被低估，具有适当输入的GAT已经强于HGNNs。提出异构图基准，标准化数据分割、特征处理、性能评估。</p></li>
<li><p>HGT较好</p></li>
<li><p>提出simple-HGNN</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="privacy-preserving">
<h2>Privacy-Preserving<a class="headerlink" href="#privacy-preserving" title="永久链接至标题"></a></h2>
<ul class="simple">
<li><p><strong>Towards Representation Identical Privacy-Preserving Graph Neural Network via Split Learning</strong></p>
<ul>
<li><p>Author: Chuanqiang Shan, Huiyun Jiao, Jie Fu</p></li>
<li><p>Publication: arXiv</p></li>
<li><p>Date: 13 Jul 2021</p></li>
<li><p>Link: <a class="reference external" href="https://arxiv.org/abs/2107.05917">https://arxiv.org/abs/2107.05917</a></p></li>
<li><p>Abstract: In recent years, the fast rise in number of studies on graph neural network (GNN) has put it from the theories research toreality application stage. Despite the encouraging performance achieved by GNN, less attention has been paid to theprivacy-preserving training and inference over distributed graph data in the related literature. Due to the particularity of graph structure,it is challenging to extend the existing private learning framework to GNN. Motivated by the idea of split learning, we propose aServerAidedPrivacy-preservingGNN(SAPGNN) for the node level task on horizontally partitioned cross-silo scenario. It offers a naturalextension of centralized GNN to isolated graph with max/min pooling aggregation, while guaranteeing that all the private data involvedin computation still stays at local data holders. To further enhancing the data privacy, a secure pooling aggregation mechanism isproposed. Theoretical and experimental results show that the proposed model achieves the same accuracy as the one learned overthe combined dat</p></li>
</ul>
</li>
<li><p><strong>Locally Private Graph Neural Networks</strong></p>
<ul>
<li><p>Author: Sina Sajadmanesh, Daniel Gatica-Perez</p></li>
<li><p>Publication: ACM CCS 2021</p></li>
<li><p>Date: 9 Jun 2020</p></li>
<li><p>Link: <a class="reference external" href="https://arxiv.org/abs/2006.05535">https://arxiv.org/abs/2006.05535</a></p></li>
<li><p>Abstract: Graph Neural Networks (GNNs) have demonstrated superior performance in learning node representations for various graph inference tasks. However, learning over graph data can raise privacy concerns when nodes represent people or human-related variables that involve sensitive or personal information. While numerous techniques have been proposed for privacy-preserving deep learning over non-relational data, there is less work addressing the privacy issues pertained to applying deep learning algorithms on graphs. In this paper, we study the problem of node data privacy, where graph nodes have potentially sensitive data that is kept private, but they could be beneficial for a central server for training a GNN over the graph. To address this problem, we develop a privacy-preserving, architecture-agnostic GNN learning algorithm with formal privacy guarantees based on Local Differential Privacy (LDP). Specifically, we propose an LDP encoder and an unbiased rectifier, by which the server can communicate with the graph nodes to privately collect their data and approximate the GNN’s first layer. To further reduce the effect of the injected noise, we propose to prepend a simple graph convolution layer, called KProp, which is based on the multi-hop aggregation of the nodes’ features acting as a denoising mechanism. Finally, we propose a robust training framework, in which we benefit from KProp’s denoising capability to increase the accuracy of inference in the presence of noisy labels. Extensive experiments conducted over real-world datasets demonstrate that our method can maintain a satisfying level of accuracy with low privacy loss.</p></li>
<li><p><a class="reference external" href="https://github.com/sisaman/lpgnn">Official Code</a></p></li>
<li><p>Note:</p></li>
</ul>
</li>
<li><p><strong>Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information</strong></p>
<ul>
<li><p>Author: Enyan Dai, Suhang Wang</p></li>
<li><p>Publication: WSDM 2021</p></li>
<li><p>Date: 3 Sep 2020</p></li>
<li><p>Link: <a class="reference external" href="https://arxiv.org/abs/2009.01454">https://arxiv.org/abs/2009.01454</a></p></li>
<li><p>Abstract: Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make predictions biased on protected sensitive attributes, e.g., skin color and gender. Because machine learning algorithms including GNNs are trained to reflect the distribution of the training data which often contains historical bias towards sensitive attributes. In addition, the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism. As a result, the applications of GNNs in sensitive domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on i.i.d data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Furthermore, the practical scenario of sparse annotations in sensitive attributes is rarely considered in existing works. Therefore, we study the novel and important problem of learning fair GNNs with limited sensitive attribute information. FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high node classification accuracy by leveraging graph structures and limited sensitive information. Our theoretical analysis shows that FairGNN can ensure the fairness of GNNs under mild conditions given limited nodes with known sensitive attributes. Extensive experiments on real-world datasets also demonstrate the effectiveness of FairGNN in debiasing and keeping high accuracy.</p></li>
<li><p>Code: <a class="reference external" href="https://github.com/EnyanDai/FairGNN">https://github.com/EnyanDai/FairGNN</a></p></li>
<li><p>Towards Representation Identical Privacy-Preserving Graph Neural Network via Split Learning</p></li>
</ul>
</li>
<li><p><strong>Graph Unlearning</strong></p>
<ul>
<li><p>Author: Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, Yang Zhang</p></li>
<li><p>Publication: arXiv</p></li>
<li><p>Date: 27 Mar 2021</p></li>
<li><p>Link: <a class="reference external" href="https://arxiv.org/abs/2103.14991">https://arxiv.org/abs/2103.14991</a></p></li>
<li><p>Abstract: The right to be forgotten states that a data subject has the right to erase their data from an entity storing it. In the context of machine learning (ML), it requires the ML model provider to remove the data subject’s data from the training set used to build the ML model, a process known as \textit{machine unlearning}. While straightforward and legitimate, retraining the ML model from scratch upon receiving unlearning requests incurs high computational overhead when the training set is large. To address this issue, a number of approximate algorithms have been proposed in the domain of image and text data, among which SISA is the state-of-the-art solution. It randomly partitions the training set into multiple shards and trains a constituent model for each shard. However, directly applying SISA to the graph data can severely damage the graph structural information, and thereby the resulting ML model utility. In this paper, we propose GraphEraser, a novel machine unlearning method tailored to graph data. Its contributions include two novel graph partition algorithms, and a learning-based aggregation method. We conduct extensive experiments on five real-world datasets to illustrate the unlearning efficiency and model utility of GraphEraser. We observe that GraphEraser achieves 2.06× (small dataset) to 35.94× (large dataset) unlearning time improvement compared to retraining from scratch. On the other hand, GraphEraser achieves up to 62.5% higher F1 score than that of random partitioning. In addition, our proposed learning-based aggregation method achieves up to 112% higher F1 score than that of the majority vote aggregation.</p></li>
</ul>
</li>
<li><p><strong>GraphMI: Extracting Private Graph Data from Graph Neural Networks</strong></p>
<ul>
<li><p>Author: Zaixi Zhang, Qi Liu, Zhenya Huang, Hao Wang, Chengqiang Lu, Chuanren Liu, Enhong Chen</p></li>
<li><p>Publication: IJCAI 2021</p></li>
<li><p>Date: 5 Jun 2021</p></li>
<li><p>Link: <a class="reference external" href="https://arxiv.org/abs/2106.02820">https://arxiv.org/abs/2106.02820</a></p></li>
<li><p>Abstract: As machine learning becomes more widely used for critical applications, the need to study its implications in privacy turns to be urgent. Given access to the target model and auxiliary information, the model inversion attack aims to infer sensitive features of the training dataset, which leads to great privacy concerns. Despite its success in grid-like domains, directly applying model inversion techniques on non-grid domains such as graph achieves poor attack performance due to the difficulty to fully exploit the intrinsic properties of graphs and attributes of nodes used in Graph Neural Networks (GNN). To bridge this gap, we present \textbf{Graph} \textbf{M}odel \textbf{I}nversion attack (GraphMI), which aims to extract private graph data of the training graph by inverting GNN, one of the state-of-the-art graph analysis tools. Specifically, we firstly propose a projected gradient module to tackle the discreteness of graph edges while preserving the sparsity and smoothness of graph features. Then we design a graph auto-encoder module to efficiently exploit graph topology, node attributes, and target model parameters for edge inference. With the proposed methods, we study the connection between model inversion risk and edge influence and show that edges with greater influence are more likely to be recovered. Extensive experiments over several public datasets demonstrate the effectiveness of our method. We also show that differential privacy in its canonical form can hardly defend our attack while preserving decent utility.</p></li>
<li><p>Official Code: https://github.com/zaixizhang/GraphMI</p></li>
</ul>
</li>
<li><p><strong>Node-Level Differentially Private Graph Neural Networks</strong></p>
<ul>
<li><p>Author: Ameya Daigavane, Gagan Madan, Aditya Sinha, Abhradeep Guha Thakurta, Gaurav Aggarwal, Prateek Jain</p></li>
<li><p>Publication:</p></li>
<li><p>Date: 23 Nov 2021</p></li>
<li><p>Link:</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2111.15521">https://arxiv.org/abs/2111.15521</a></p></li>
<li><p><a class="reference external" href="https://openreview.net/forum?id=tCx6AefvuPf">https://openreview.net/forum?id=tCx6AefvuPf</a></p></li>
</ul>
</li>
<li><p>Abstract: Graph Neural Networks (GNNs) are a popular technique for modelling graph-structured data that compute node-level representations via aggregation of information from the local neighborhood of each node. However, this aggregation implies increased risk of revealing sensitive information, as a node can participate in the inference for multiple nodes. This implies that standard privacy preserving machine learning techniques, such as differentially private stochastic gradient descent (DP-SGD) - which are designed for situations where each data point participates in the inference for one point only - either do not apply, or lead to inaccurate solutions. In this work, we formally define the problem of learning 1-layer GNNs with node-level privacy, and provide an algorithmic solution with a strong differential privacy guarantee. Even though each node can be involved in the inference for multiple nodes, by employing a careful sensitivity analysis anda non-trivial extension of the privacy-by-amplification technique, our method is able to provide accurate solutions with solid privacy parameters. Empirical evaluation on standard benchmarks demonstrates that our method is indeed able to learn accurate privacy preserving GNNs, while still outperforming standard non-private methods that completely ignore graph information.</p></li>
</ul>
</li>
</ul>
<!--
- ****
  - Author: 
  - Publication: 
  - Date: 
  - Link: <>
  - Abstract:
--><!-- 
Shirui Pan's Lab
- [ICDM 2019 | Relation Structure-Aware Heterogeneous Graph Neural Network](https://shiruipan.github.io/publication/icdm-19-zhu/)
- [IEEE Transactions on Neural Networks and Learning Systems | A comprehensive survey on graph neural networks](https://shiruipan.github.io/publication/wu-2019-comprehensive/)
- [IJCAI-20 | Reasoning Like Human: Hierarchical Reinforcement Learning for Knowledge Graph Reasoning](https://shiruipan.github.io/publication/ijcai-2020-wan/)
- [JCDL-20 | Multivariate Relations Aggregation Learning in Social Networks](https://shiruipan.github.io/publication/jcdl-2020-xu/)
- [PAKDD-21 | Heterogeneous Graph Attention Network for Small and Medium-Sized Enterprises Bankruptcy Prediction](https://shiruipan.github.io/publication/pakdd-21-zheng/)
- [A survey on knowledge graphs: representation, acquisition, and applications](https://shiruipan.github.io/publication/tnnls-21-ji/)
- [Graph Learning: A Survey](https://shiruipan.github.io/publication/tai-21-xia/)
- [A Survey of Community Detection Approaches: From Statistical Modeling to Deep Learning](https://shiruipan.github.io/publication/tkde-jin-21/)
- [Graph self-supervised learning: A survey](https://shiruipan.github.io/publication/liu-21-survey/) 
- --></section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../FL.html" class="btn btn-neutral float-left" title="Federated Graph Learning Resources" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="FGL.html" class="btn btn-neutral float-right" title="Federated Graph Learning Paper List" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2021, Hongyu.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>